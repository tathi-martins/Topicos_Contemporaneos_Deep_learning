{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hT_eVn0PDLIM",
        "outputId": "1eb5d5e7-d29b-4ffa-ad76-72fbcfdfb32a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting openai<2.0.0,>=1.58.1 (from langchain_openai)\n",
            "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_openai) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain_openai) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n",
            "Downloading langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-core, langchain_openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.57.4\n",
            "    Uninstalling openai-1.57.4:\n",
            "      Successfully uninstalled openai-1.57.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "Successfully installed langchain-core-0.3.28 langchain_openai-0.2.14 openai-1.58.1 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4nPPub2CllU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74217fc1-9362-4479-c27b-8d43bd3a537a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiDjOrKxCllV"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt2d1RbOCllW"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh2iq0jACllX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21839b2b-9634-4e0a-b3f2-4c87160a81cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá! Como posso ajudar você hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-93804673-e7eb-4bbe-8135-6258f5557357-0' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "# Runnable\n",
        "\n",
        "response = llm.invoke(\"Olá\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHmb7-qlCllX"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PJMFIQxCllX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_ds63RSCllX"
      },
      "source": [
        "### Templates Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMyNbkuICllX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f0ae69-1695-445e-9b7e-b67785fe36af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para português: Artificial Intelligence is the future!', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para português: {texto}\")\n",
        "\n",
        "prompt = prompt_template.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrftWD9jCllY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f129c397-b666-4e92-f8de-9e3ad339afb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Inteligência Artificial é o futuro!\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w87TCHZCllY"
      },
      "source": [
        "### Templates de Mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIKrvQvVCllY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c666bc-75f7-4287-d628-ec96a8646ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá, como você está?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 36, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-8eff7c20-2740-43b1-912b-6f0cb059a552-0' usage_metadata={'input_tokens': 36, 'output_tokens': 7, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DdjCrnOCllY"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaO2-DigCllZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "850e3e9b-4012-4576-a875-4175262e3097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"português\",\n",
        "    \"texto\": \"Hello, how are you?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WTJ6E9UCllZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee9fbc3-bcbf-4fa7-831b-0fca3efa8004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá, como você está?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilSRLyjECllZ"
      },
      "source": [
        "### Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOPrv5_UCllZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752e8f81-8570-4eb2-989b-262491c497f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='A capital do Rio Grande do Norte é Natal.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 16, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-65834f96-dc32-42b1-be19-09f980c72135-0' usage_metadata={'input_tokens': 16, 'output_tokens': 11, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Saída do parser:\n",
            "A capital do Rio Grande do Norte é Natal.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "str_parser = StrOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Qual a capital do Rio Grande do Norte?\")\n",
        "output = str_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Saída do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cLSry3SClla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a39f755-f9f5-4837-de20-81b2eb12914e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='Aqui está a representação em formato JSON das massas e cargas das partículas que constituem o átomo:\\n\\n```json\\n{\\n  \"próton\": {\\n    \"massa\": \"1.6726 x 10^-27 kg\",\\n    \"carga\": \"+1e\"\\n  },\\n  \"nêutron\": {\\n    \"massa\": \"1.6750 x 10^-27 kg\",\\n    \"carga\": \"0e\"\\n  },\\n  \"elétron\": {\\n    \"massa\": \"9.1094 x 10^-31 kg\",\\n    \"carga\": \"-1e\"\\n  }\\n}\\n```\\n\\nNeste formato, cada partícula é uma chave, e os valores associados são objetos contendo a massa e a carga da partícula.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 38, 'total_tokens': 194, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-11369eef-600b-4790-9dd3-85f756a768e5-0' usage_metadata={'input_tokens': 38, 'output_tokens': 156, 'total_tokens': 194, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Saída do parser:\n",
            "{'próton': {'massa': '1.6726 x 10^-27 kg', 'carga': '+1e'}, 'nêutron': {'massa': '1.6750 x 10^-27 kg', 'carga': '0e'}, 'elétron': {'massa': '9.1094 x 10^-31 kg', 'carga': '-1e'}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Quais as massas e cargas das partículas que constituem o átomo? Responda no formato JSON em que cada chave seja o nome da partícula\")\n",
        "output = json_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Saída do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saída do parser:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DmFAhNrGJLq",
        "outputId": "05cdae74-7aa1-4bf1-a948-bc6613ff9a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saída do parser:\n",
            "{'próton': {'massa': '1.6726 x 10^-27 kg', 'carga': '+1e'}, 'nêutron': {'massa': '1.6750 x 10^-27 kg', 'carga': '0e'}, 'elétron': {'massa': '9.1094 x 10^-31 kg', 'carga': '-1e'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output['elétron']['carga']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rieWg77gGKwa",
        "outputId": "314251e8-f194-4f5c-f3cd-594f64aa7394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'-1e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nJV54hvClla"
      },
      "source": [
        "## Encadeamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiRscy3BClla"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQo2CetCClla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5589410e-25d2-43e6-b439-733e96751325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubarões!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znK01rkwClla"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEjy_GCQClla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1153bd85-abba-4c0b-9481-20dcd440c1cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"inglês\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IhKUwtBCllb"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbW8iiJzCllb"
      },
      "source": [
        "### Exercício 1\n",
        "Crie uma `chain` que a partir de um tópico informado pelo usuário, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HivcdhM1Cllb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d21f1ad7-18d2-430f-c5ad-da927a315c6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Por que o livro de matemática se suicidou?\\n\\nPorque tinha muitos problemas!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"Crie uma piada sobre {tema}\")\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "chain.invoke({'tema': 'educação'})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeira resposta gerada:\n",
        "\n",
        "**Por que o livro de matemática se suicidou?**\n",
        "\n",
        "**Porque ele tinha muitos problemas!**\n",
        "\n",
        "KKKK até que foi engraçada"
      ],
      "metadata": {
        "id": "m4PdhJI4ISVQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aXJqN6jCllb"
      },
      "source": [
        "### Exercício 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6737mrOuCllb"
      },
      "outputs": [],
      "source": [
        "chain_2 = llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def identifica_sentimentos(texto):\n",
        "    response = chain_2.invoke(\"Classifique o texto a seguir em positivo, neutro ou negativo: \" + texto)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "sUc9nfbGJBVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = identifica_sentimentos(\"The beaches of Recife have sharks!\")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAOugH-cJ5CC",
        "outputId": "b822525b-1b27-4f33-f721-c6cdedfb031f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O texto pode ser classificado como negativo, pois menciona a presença de tubarões nas praias, o que pode gerar preocupação ou medo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_2 = identifica_sentimentos(\"A derivada de uma função descreve a taxa de variação instantânea da função em um certo ponto.\")\n",
        "\n",
        "print(output_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nhLnPXhMuv1",
        "outputId": "f121c400-cd11-4c4b-a590-60c8a5c94461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O texto pode ser classificado como neutro. Ele apresenta uma explicação técnica e objetiva sobre o conceito de derivada, sem emitir juízo de valor ou opinião.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_3 = identifica_sentimentos(\"Acredite em si próprio e todo o resto estará em seu caminho.\")\n",
        "\n",
        "print(output_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejM5jNZ1NAfm",
        "outputId": "eef77e39-c62e-460e-ed06-5568bd71ab2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O texto pode ser classificado como positivo. Ele transmite uma mensagem de encorajamento e autoconfiança.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsLbXB5dCllb"
      },
      "source": [
        "### Exercício 3\n",
        "Crie uma `chain` que gere o código de uma função Python de acordo com a descrição do usuário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwF4FGUsCllb"
      },
      "outputs": [],
      "source": [
        "chain_2 = llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gera_codigos(texto):\n",
        "    response = chain_2.invoke(\"Gere o código de uma função Python de acordo com a descrição do usuário, adicione comentários sobre a função\" + texto)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "-I9pmeddN01s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_codigo = gera_codigos(\"crie uma função que faça calculos de uma calculadora básica. Mantenha a resposta simples e concisa\")\n",
        "\n",
        "print(output_codigo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAHpjRlIOA_T",
        "outputId": "c5c83987-e4ac-4142-bbf0-e4189dd596f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aqui está uma função em Python que realiza operações básicas de uma calculadora:\n",
            "\n",
            "```python\n",
            "def calculadora(num1, num2, operacao):\n",
            "    \"\"\"\n",
            "    Função que realiza cálculos básicos: adição, subtração, multiplicação e divisão.\n",
            "    \n",
            "    Parâmetros:\n",
            "    num1 (float): Primeiro número.\n",
            "    num2 (float): Segundo número.\n",
            "    operacao (str): A operação a ser realizada ('+', '-', '*', '/').\n",
            "    \n",
            "    Retorna:\n",
            "    float: O resultado da operação.\n",
            "    \"\"\"\n",
            "    if operacao == '+':\n",
            "        return num1 + num2\n",
            "    elif operacao == '-':\n",
            "        return num1 - num2\n",
            "    elif operacao == '*':\n",
            "        return num1 * num2\n",
            "    elif operacao == '/':\n",
            "        if num2 == 0:\n",
            "            return \"Erro: Divisão por zero!\"\n",
            "        return num1 / num2\n",
            "    else:\n",
            "        return \"Erro: Operação inválida!\"\n",
            "\n",
            "# Exemplo de uso\n",
            "resultado = calculadora(10, 5, '+')\n",
            "print(resultado)  # Saída: 15\n",
            "```\n",
            "\n",
            "### Comentários sobre a função:\n",
            "- A função `calculadora` aceita dois números e uma operação como entrada.\n",
            "- Ela verifica qual operação deve ser realizada e retorna o resultado correspondente.\n",
            "- Há tratamento para divisão por zero e operações inválidas, garantindo que a função seja robusta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv3qK8RbCllb"
      },
      "source": [
        "### Exercício 4\n",
        "Crie uma `chain` que explique de forma simplificada um tópico geral fornecido pelo usuário e, em seguida, traduza a explicação para inglês. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_4 = llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "gUGIvr3hWXhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generates_text(texto):\n",
        "    response = chain_4.invoke(\"Explique de forma simplificada usando dois parágrafos um tema fornecido pelo usuário\" + texto)\n",
        "    return response\n",
        "\n",
        "def explain_and_translate(text):\n",
        "    response = chain_4.invoke(\"Traduza para o inglês\" + generates_text(text))\n",
        "    return response\n",
        "\n",
        "\n",
        "output = explain_and_translate(\"O que é Hazmat\")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "SbBdfU77RcNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ce0939-38ed-4f3f-eafc-913bbebff2c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hazmat, short for \"hazardous materials,\" refers to substances that pose risks to human health, the environment, or safety during transportation, handling, or storage. These materials can include chemical, biological, or physical substances that, if not properly managed, can cause significant harm. Common examples of Hazmat include toxic, flammable, corrosive, and reactive substances, such as pesticides, industrial solvents, and certain types of waste.\n",
            "\n",
            "Hazmat management is essential to ensure public safety and environmental protection. This involves strict regulations regarding the transportation and disposal of these substances, as well as the implementation of proper handling and storage practices. Trained professionals, such as firefighters and emergency response teams, are often called upon to deal with incidents involving hazardous materials, ensuring that risks are minimized and that the health of people and the ecosystem is preserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, chain):\n",
        "\n",
        "        self.chain = chain\n",
        "\n",
        "    def generates_text(self, topic):\n",
        "\n",
        "        prompt = \"Explique de forma simplificada usando dois parágrafos um tema fornecido pelo usuário: \" + topic\n",
        "        response = self.chain.invoke(prompt)\n",
        "        return response\n",
        "\n",
        "    def explain_and_translate(self, topic):\n",
        "\n",
        "        explanation = self.generates_text(topic)\n",
        "        prompt = \"Traduza para o inglês: \" + explanation\n",
        "        response = self.chain.invoke(prompt)\n",
        "        return response\n"
      ],
      "metadata": {
        "id": "YzY8Ks3rS4CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = TextProcessor(chain_4)\n",
        "\n",
        "output = processor.explain_and_translate(\"O que é Hazmat\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhomL7qiiCsB",
        "outputId": "d1b60173-48c4-4adb-cee6-75c1b66e98e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hazmat, or hazardous material, refers to substances that pose risks to health, safety, property, or the environment. These materials can include chemical, biological, and radioactive substances that, if not handled, stored, or disposed of properly, can cause significant harm. Examples of hazmat include industrial chemicals, fuels, pesticides, and medical waste. It is crucial that these materials are identified and classified to ensure they are treated safely.\n",
            "\n",
            "Hazmat management involves a series of regulations and practices aimed at minimizing the risks associated with their use and transport. This includes training for professionals who handle these materials, the use of appropriate protective equipment, and compliance with environmental standards. Additionally, there are specific procedures for emergency response in case of spills or accidents, aimed at protecting both people and the surrounding environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYw3lZ22Cllb"
      },
      "source": [
        "### Exercício 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H18m3JauCllc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbbbf6a9-8b6f-4362-96a6-e6ffc0acf1a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**CESAR School: A Revolução na Educação em Tecnologia e Inovação**\n",
            "\n",
            "Recife, PE - A CESAR School, uma das instituições de ensino mais reconhecidas do Brasil na área de tecnologia e inovação, vem se destacando como um verdadeiro polo de formação de profissionais capacitados para enfrentar os desafios do mercado contemporâneo. Com um currículo que une teoria e prática, a escola, vinculada ao Centro de Estudos e Sistemas Avançados de Recife (CESAR), oferece cursos de graduação e pós-graduação que têm atraído a atenção de estudantes e empresas em todo o país.\n",
            "\n",
            "Fundada em 2015, a CESAR School se propõe a ir além do ensino tradicional, promovendo uma abordagem centrada no desenvolvimento de habilidades práticas e no estímulo ao pensamento crítico. Seus cursos, que incluem áreas como Design, Ciência da Computação, Engenharia de Software e Gestão de Inovação, são elaborados em colaboração com profissionais da indústria, garantindo que o conteúdo esteja alinhado às demandas do mercado.\n",
            "\n",
            "Um dos diferenciais da CESAR School é a metodologia de ensino baseada em projetos. Os alunos têm a oportunidade de trabalhar em desafios reais, desenvolvendo soluções inovadoras para problemas enfrentados por empresas parceiras. Essa experiência prática não apenas enriquece o aprendizado, mas também facilita a inserção dos graduados no mercado de trabalho, que frequentemente busca profissionais com experiência prática e habilidades colaborativas.\n",
            "\n",
            "O corpo docente da CESAR School é composto por especialistas reconhecidos em suas áreas, muitos dos quais atuam em projetos de pesquisa e desenvolvimento no CESAR. Essa conexão com o mundo acadêmico e empresarial proporciona aos alunos uma visão abrangente e atualizada sobre as tendências e tecnologias emergentes.\n",
            "\n",
            "Além da formação acadêmica, a CESAR School também se destaca por seu ecossistema de inovação. A instituição promove eventos, hackathons e workshops que incentivam a troca de ideias e a criação de redes de contato entre alunos, ex-alunos e profissionais da indústria. Essas iniciativas têm contribuído para o fortalecimento do ambiente de inovação em Recife, consolidando a cidade como um importante hub tecnológico do Brasil.\n",
            "\n",
            "Com uma proposta educacional inovadora e uma forte conexão com o mercado, a CESAR School tem se consolidado como uma referência no ensino de tecnologia e inovação. À medida que o mundo avança rapidamente em direção à digitalização e à inovação, a instituição se posiciona como uma poderosa aliada na formação de profissionais prontos para liderar essa transformação.\n"
          ]
        }
      ],
      "source": [
        "prompt_template_aboutcesar = ChatPromptTemplate.from_template(\"Escreva um texto estilo jornalístico sobre o CESAR School\")\n",
        "chain_5 = prompt_template_aboutcesar | llm | StrOutputParser()\n",
        "resposta = chain_5.invoke({})\n",
        "saida = resposta\n",
        "print(saida)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}